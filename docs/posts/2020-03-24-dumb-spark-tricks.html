<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Huw Campbell - Dumb Spark Tricks - Partitioning</title>
  <meta name="author" content="Huw Campbell">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="../css/normalize.css">
  <link rel="stylesheet" href="../css/skeleton.css">
  <link rel="stylesheet" href="../css/syntax.css">
  <link rel="stylesheet" href="../css/custom.css">

  <!-- Me -->
  <link rel="me" href="https://github.com/HuwCampbell">
  <link rel="me" href="https://bsky.app/profile/huwcampbell.com">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon.png">

  <!-- ATOM Feed
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="../atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  <link rel="site.standard.document" href="at://did:plc:6e7msnu5o3mdboitzv7oxbxm/site.standard.document/3mdos57uqmf2j">
  </head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="three columns sidebar">
        <nav>
            <a href="../"><img class="titular" src="../images/pirate.jpg" alt="Huw Campbell"></a>
            <h3 id="logo">Huw Campbell</h3>
            <ul>
                <li><a href="../">Home</a></li>
                <li><a href="../about.html">About</a></li>
                <li><a href="../archive.html">Posts</a></li>
                <li><a href="../resume.html">Resume</a></li>
                <li><a href="https://github.com/HuwCampbell">Github</a></li>
                <li><a href="https://tangled.org/@huwcampbell.com">Tangled</a></li>       
            </ul>
        </nav>
        &nbsp;
    </div>

    <div class="nine columns content">
        <h1>Dumb Spark Tricks - Partitioning</h1>

        <div class="info">
    Posted on March 24, 2020
    
    
</div>

<p>I’ve been working with Spark for a few years now, and have pushed it into realms where it was not
really meant to go. Over this time, I have written a lot of custom aggregators, catalyst expressions,
custom RDDs and streaming interfaces.</p>
<p>This series is to share some of the interesting, painful, and downright silly things which I have had
been forced to do.</p>
<p>We’re going to kick off with a relatively simple one.</p>
<h4 id="how-to-trick-spark-into-not-doing-any-repartitioning-when-we-know-the-data-layout">How to trick Spark into not doing any repartitioning when we know the data layout</h4>
<p>I needed to join two datasets, but I knew my input data (from Kafka) was partitioned in a way that meant
I could union and join without a shuffle step. In my use case the data volumes were legitimately large,
and were coming in streaming. Any delay based on shuffling data between nodes would mean we would fall
behind and probably never catch up.</p>
<p>Fortunately, after my colleague and I requested it, our input data was partitioned using the same join
key; unfortunately, Spark couldn’t know that in advance.</p>
<p>For various reasons, we were using RDDs instead of Dataframes, and the solution I came up with was to
wrap the Kafka RDDs inside new ones, which enforced a custom “never ever shuffle” partitioner.</p>
<p>The API for writing RDDs looks something like this:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode scala"><code class="sourceCode scala"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">abstract</span> <span class="kw">class</span> RDD<span class="op">[</span>T<span class="op">:</span> ClassTag<span class="op">](</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">private</span> <span class="kw">var</span> _sc<span class="op">:</span> SparkContext<span class="op">,</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">private</span> <span class="kw">var</span> deps<span class="op">:</span> <span class="bu">Seq</span><span class="op">[</span>Dependency<span class="op">[</span>_<span class="op">]]</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Just a helper function for building single child RDDs</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="kw">this</span><span class="op">(</span>oneParted<span class="op">:</span> RDD<span class="op">[</span>_<span class="op">])</span> <span class="op">=</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>      <span class="kw">this</span><span class="op">(</span>oneParent<span class="op">.</span>context<span class="op">,</span> <span class="ex">List</span><span class="op">(</span><span class="kw">new</span> <span class="fu">OneToOneDependency</span><span class="op">(</span>oneParent<span class="op">)))</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">compute</span><span class="op">(</span>split<span class="op">:</span> Partition<span class="op">,</span> context<span class="op">:</span> TaskContext<span class="op">):</span> <span class="ex">Iterator</span><span class="op">[</span>T<span class="op">]</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">protected</span> <span class="kw">def</span> getPartitions<span class="op">:</span> <span class="ex">Array</span><span class="op">[</span>Partitions<span class="op">]</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">val</span> partitioner<span class="op">:</span> <span class="ex">Option</span><span class="op">[</span>Partitioner<span class="op">]</span> <span class="op">=</span> <span class="bu">None</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span></code></pre></div>
<p>What we want to do it create a new RDD, which acts just like the old one,
but enforcing a custom partitioner; and this is actually really pretty simple:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode scala"><code class="sourceCode scala"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">case</span> <span class="kw">class</span> UsingPartitioner<span class="op">[</span>T<span class="op">:</span> ClassTag<span class="op">](</span>parent<span class="op">:</span> RDD<span class="op">[</span>T<span class="op">],</span> _partitioner<span class="op">:</span> Partitioner<span class="op">)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">extends</span> RDD<span class="op">[</span>T<span class="op">](</span>parent<span class="op">)</span> <span class="op">{</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">override</span> <span class="kw">def</span> <span class="fu">compute</span><span class="op">(</span>split<span class="op">:</span> Partition<span class="op">,</span> context<span class="op">:</span> TaskContext<span class="op">):</span> <span class="ex">Iterator</span><span class="op">[</span>T<span class="op">]</span> <span class="op">=</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    parent<span class="op">.</span><span class="fu">compute</span><span class="op">(</span>split<span class="op">,</span> context<span class="op">)</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="kw">override</span> <span class="kw">protected</span> <span class="kw">def</span> getPartitions<span class="op">:</span> <span class="ex">Array</span><span class="op">[</span>Partitions<span class="op">]</span> <span class="op">=</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    parent<span class="op">.</span>partitions</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="kw">override</span> <span class="kw">val</span> partitioner<span class="op">:</span> <span class="ex">Option</span><span class="op">[</span>Partitioner<span class="op">]</span> <span class="op">=</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">Some</span><span class="op">(</span>_partitioner<span class="op">)</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>Now, to make sure that Spark is never going to perform a shuffle, we can write a little
partitioner which is just a dummy, but makes sure than any attempts to shuffle will
throw an exception.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode scala"><code class="sourceCode scala"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="cf">case</span> <span class="kw">class</span> <span class="fu">FreezeNode</span><span class="op">(</span>numNode<span class="op">:</span> <span class="bu">Int</span><span class="op">)</span> <span class="kw">extends</span> Partitioner <span class="op">{</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> numPartitions<span class="op">:</span> <span class="bu">Int</span> <span class="op">=</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    numNode</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">getPartition</span><span class="op">(</span>key<span class="op">:</span> <span class="ex">Any</span><span class="op">):</span> <span class="bu">Int</span> <span class="op">=</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">throw</span> <span class="kw">new</span> <span class="ex">Exception</span><span class="op">(</span><span class="st">&quot;Spark is trying to perform a shuffle!&quot;</span><span class="op">)</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>Now we are able to, if we know our partitioning strategy in advance, write joins which
can not repartition the data and guarantee efficient joins.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode scala"><code class="sourceCode scala"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">val</span> frozenLeft<span class="op">:</span> RDD<span class="op">[(</span>K<span class="op">,</span> W<span class="op">)]</span> <span class="op">=</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">UsingPartitioner</span><span class="op">(</span><span class="fu">FreezeNode</span><span class="op">(</span><span class="dv">5</span><span class="op">)),</span> left<span class="op">)</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">val</span> frozenRight RDD<span class="op">[(</span>K<span class="op">,</span> X<span class="op">)]</span> <span class="op">=</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">UsingPartitioner</span><span class="op">(</span><span class="fu">FreezeNode</span><span class="op">(</span><span class="dv">5</span><span class="op">)),</span> right<span class="op">)</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">val</span> joined<span class="op">:</span> RDD<span class="op">[(</span>K<span class="op">,</span> <span class="op">(</span>W<span class="op">,</span> X<span class="op">))]=</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  frozenLeft join frozenRight</span></code></pre></div>
<p>We know that spark can not have performed a shuffle when we request data from
the joined dataset. Obviously, correctness is now up to us, as if our partitioning
is wrong, we’ll certainly and silently drop data.</p>
<h4 id="join-free-streaming-applications">Join Free Streaming Applications</h4>
<p>For Streaming applications Spark uses DStreams as the base class. These are <em>relatively</em>
simple wrappers over RDDs, and one can use <code>transform</code> to wrap the internally computed
RDDs with the frozen partitioner.</p>
<h4 id="conclusion">Conclusion</h4>
<p>Spark provides optimisations which can make safe joins more efficient, but we can hijack
them if we really need to to make our unsafe joins also fast.</p>
<p>Join me next time, when we’ll talk about Streaming applications, and how to deal with
funky time discrepancies.</p>

    </div>
  </div>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>